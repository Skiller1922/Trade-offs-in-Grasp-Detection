{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Create dataset: sequence of preporcessed examples ready to feed to neuralnet \n",
    "2. Create dataloader: define how dataset is loaded to neuralnet (batch size, order, computation optimizing ...)\n",
    "3. Create model : a bunch of matrixes math to transform input tensor to output tensor\n",
    "4. Training loop:\n",
    "    + Forward \n",
    "    + Calculate loss\n",
    "    + Backward\n",
    "    + Monitoring: \n",
    "        + Evaluate metrics\n",
    "        + Logger, back and forth\n",
    "        + Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2data = \"../data/interim/grasp/cornell/\"\n",
    "\n",
    "def _process_bboxes(name):\n",
    "    '''Create a list with the coordinates of the grasping rectangles. Every \n",
    "    element is either x or y of a vertex.'''\n",
    "    with open(name, 'r') as f:\n",
    "        bboxes = list(map(\n",
    "              lambda coordinate: float(coordinate), f.read().strip().split()))\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def create_image_list():\n",
    "    # Creating a list with all the image paths\n",
    "    folders = range(1,11)\n",
    "    folders = ['0'+str(i) if i<10 else '10' for i in folders]\n",
    "    filenames = []\n",
    "    for i in folders:\n",
    "        for name in glob.glob(os.path.join(path2data, i, 'pcd'+i+'*r.png')):\n",
    "            filenames.append(name)\n",
    "    return filenames\n",
    "\n",
    "def load_img_label(filename):    \n",
    "    label_file = filename[:-5]+'cpos.txt'\n",
    "    bboxes = _process_bboxes(label_file)\n",
    "    label = np.array(bboxes)\n",
    "    label = label.reshape(-1,8)\n",
    "    label_clean = []        \n",
    "    for box in label:\n",
    "        if not np.isnan(box).any():\n",
    "            label_clean.append(box.tolist())\n",
    "    label_clean = np.array(label_clean).reshape(-1).tolist()          \n",
    "    img = Image.open(filename)\n",
    "    return img,label_clean\n",
    "\n",
    "def show_img_label(img,label, thickness=2, size=50):  \n",
    "    img = img.copy()\n",
    "    plt.figure(figsize=(size, size))\n",
    "    # draw a plolygon\n",
    "    draw = ImageDraw.Draw(img) \n",
    "    draw.polygon(label[-8:], outline=\"blue\")    \n",
    "    plt.imshow(np.asarray(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cornell_Grasp_dataset(Dataset):\n",
    "    def __init__(self, path2data, transform, trans_params):      \n",
    "        pass    \n",
    "      \n",
    "    def __len__(self):\n",
    "        # return size of dataset\n",
    "        return len(self.fullPath2img)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, path2data, transform=None, trans_params=None):      \n",
    "    # Creating a list with all the image paths\n",
    "    folders = range(1,11)\n",
    "    folders = ['0'+str(i) if i<10 else '10' for i in folders]\n",
    "    self.fullPath2img = []\n",
    "    for i in folders:\n",
    "        for name in glob.glob(os.path.join(path2data, i, 'pcd'+i+'*r.png')):\n",
    "            self.fullPath2img.append(name)\n",
    "    self.transform = transform\n",
    "    self.trans_params=trans_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def __getitem__(self, idx):\n",
    "    # load PIL image  \n",
    "    image, label = load_img_label(self.fullPath2img[idx])\n",
    "    label = torch.tensor(label)\n",
    "    # transform to tensor\n",
    "    if self.transform:\n",
    "        image,label = self.transform(image,label,self.trans_params)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cornell_Grasp_dataset.__init__=__init__\n",
    "Cornell_Grasp_dataset.__getitem__=__getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def resize_img_label(image,label,target_size=(256,256)):\n",
    "    w_orig,h_orig = image.size   \n",
    "    w_target,h_target = target_size\n",
    "    label = label.view(-1,2)\n",
    "    # resize image and label\n",
    "    image_new = TF.resize(image,target_size)\n",
    "    for i in range(len(label)):\n",
    "        x, y = label[i]\n",
    "        label[i][0] = x/w_orig*w_target\n",
    "        label[i][1] = y/h_orig*h_target     \n",
    "    label = label.view(-1,8)\n",
    "    return image_new,label\n",
    "\n",
    "def transformer(image, label, params):\n",
    "    image,label=resize_img_label(image,label,params[\"target_size\"]) \n",
    "    if params[\"sample_output\"]:\n",
    "        # randoom choose a grasp to be the ground truth      \n",
    "        index = random.randint(0, len(label) -1)\n",
    "        label = label[index]  \n",
    "    image=TF.to_tensor(image)\n",
    "    return image, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, labels = list(zip(*batch))\n",
    "    targets = []\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        target = torch.zeros(label.shape[0], label.shape[1] + 1)\n",
    "        target[:,0] = i\n",
    "        target[:, 1:] = label\n",
    "        targets.append(target)\n",
    "\n",
    "    targets = torch.cat(targets, 0)\n",
    "    imgs = torch.stack([img for img in imgs])\n",
    "    return imgs, targets, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test transformer\n",
    "import random\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "params = {\n",
    "    \"target_size\" : (256, 256),\n",
    "    \"sample_output\" : True\n",
    "}\n",
    "\n",
    "filenames = create_image_list()\n",
    "f1 = filenames[42]\n",
    "\n",
    "img, label = load_img_label(f1)\n",
    "\n",
    "show_img_label(img,label)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t,label_t=transformer(img, torch.tensor(label).view(-1,2),params)\n",
    "\n",
    "print(label_t)\n",
    "label_t = torch.squeeze(label_t.view(1,-1)).tolist()\n",
    "\n",
    "show_img_label(TF.to_pil_image(img_t),label_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_params_train={\n",
    "    \"target_size\" : (256, 256),\n",
    "    \"sample_output\" : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trans_params_val={\n",
    "    \"target_size\" : (256, 256), \n",
    "    \"sample_output\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = Cornell_Grasp_dataset(path2data,transformer,trans_params_train)\n",
    "val_ds = Cornell_Grasp_dataset(path2data,transformer,trans_params_val)\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "sss = ShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "indices=range(len(train_ds))\n",
    "\n",
    "for train_index, val_index in sss.split(indices):\n",
    "    print(len(train_index))\n",
    "    print(\"-\"*10)\n",
    "    print(len(val_index))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "train_ds = Subset(train_ds,train_index)\n",
    "print(len(train_ds))\n",
    "\n",
    "val_ds = Subset(val_ds,val_index)\n",
    "print(len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "def show(img,label=None):\n",
    "    npimg = img.numpy().transpose((1,2,0))\n",
    "    plt.imshow(npimg)\n",
    "    if label is not None:\n",
    "        label = label.view(-1,2)\n",
    "        for point in label:\n",
    "            x,y= point\n",
    "            plt.plot(x,y,'b+',markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for img,label in train_ds:\n",
    "    show(img,label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for img,label in val_ds:\n",
    "    show(img,label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=266, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_b, label_b in train_dl:\n",
    "    print(img_b.shape,img_b.dtype)\n",
    "    print(label_b.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in val_dl:\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, params):\n",
    "    super(Net, self).__init__()\n",
    "\n",
    "    C_in,H_in,W_in=params[\"input_shape\"]\n",
    "    init_f=params[\"initial_filters\"] \n",
    "    num_outputs=params[\"num_outputs\"] \n",
    "\n",
    "    self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3, stride=2, padding=1)\n",
    "    self.conv2 = nn.Conv2d(init_f+C_in, 2*init_f, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3 = nn.Conv2d(3*init_f+C_in, 4*init_f, kernel_size=3, padding=1)\n",
    "    self.conv4 = nn.Conv2d(7*init_f+C_in, 8*init_f, kernel_size=3, padding=1)\n",
    "    self.conv5 = nn.Conv2d(15*init_f+C_in, 16*init_f, kernel_size=3, padding=1)\n",
    "    # self.conv6 = nn.Conv2d(31*init_f+C_in, 32*init_f, kernel_size=3, padding=1)\n",
    "    # self.conv7 = nn.Conv2d(63*init_f+C_in, 64*init_f, kernel_size=3, padding=1)\n",
    "    # self.conv8 = nn.Conv2d(127*init_f+C_in, 128*init_f, kernel_size=3, padding=1)\n",
    "    self.fc1 = nn.Linear(16*init_f, num_outputs)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    identity=F.avg_pool2d(x,4,4)\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = F.max_pool2d(x, 2, 2)\n",
    "    x = torch.cat((x, identity), dim=1)\n",
    "    #print(\"After conv1:\", x.shape)\n",
    "\n",
    "    identity=F.avg_pool2d(x,2,2)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.max_pool2d(x, 2, 2)\n",
    "    x = torch.cat((x, identity), dim=1)\n",
    "    # #print(\"After conv2:\", x.shape)\n",
    "\n",
    "    identity=F.avg_pool2d(x,2,2)\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x = F.max_pool2d(x, 2, 2)\n",
    "    x = torch.cat((x, identity), dim=1)\n",
    "    # #print(\"After conv3:\", x.shape)\n",
    "    \n",
    "    identity=F.avg_pool2d(x,2,2)\n",
    "    x = F.relu(self.conv4(x))\n",
    "    x = F.max_pool2d(x, 2, 2)\n",
    "    x = torch.cat((x, identity), dim=1)\n",
    "    # #print(\"After conv4:\", x.shape)\n",
    "\n",
    "    # identity=F.avg_pool2d(x,2,2)\n",
    "    x = F.relu(self.conv5(x))\n",
    "    # x = F.max_pool2d(x, 2, 2)\n",
    "    # x = torch.cat((x, identity), dim=1)\n",
    "    # # # # #print(\"After conv5:\", x.shape)\n",
    "\n",
    "    # identity=F.avg_pool2d(x,2,2)\n",
    "    # x = F.relu(self.conv6(x))\n",
    "    # x = F.max_pool2d(x, 2, 2)\n",
    "    # x = torch.cat((x, identity), dim=1)\n",
    "    # # # # #print(\"After conv6:\", x.shape)\n",
    "\n",
    "    # identity=F.avg_pool2d(x,2,2)\n",
    "    # x = F.relu(self.conv7(x))\n",
    "    # x = F.max_pool2d(x, 2, 2)\n",
    "    # x = torch.cat((x, identity), dim=1)\n",
    "\n",
    "    # x = F.relu(self.conv8(x))\n",
    "    x=F.adaptive_avg_pool2d(x,1)\n",
    "    x = x.reshape(x.size(0), -1)\n",
    "    x = self.fc1(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net.__init__=__init__\n",
    "Net.forward=forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_model={\n",
    "        \"input_shape\": (3,256,256),\n",
    "        \"initial_filters\": 16, \n",
    "        \"num_outputs\": 5,\n",
    "            }\n",
    "\n",
    "model = Net(params_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform function \n",
    "def bboxes_to_grasps(bboxes):\n",
    "    # convert bbox to grasp representation -> tensor([x, y, theta, h, w])\n",
    "    x = bboxes[:,0] + (bboxes[:,4] - bboxes[:,0])/2\n",
    "    y = bboxes[:,1] + (bboxes[:,5] - bboxes[:,1])/2 \n",
    "    theta = torch.atan((bboxes[:,3] -bboxes[:,1]) / (bboxes[:,2] -bboxes[:,0]))\n",
    "    w = torch.sqrt(torch.pow((bboxes[:,2] -bboxes[:,0]), 2) + torch.pow((bboxes[:,3] -bboxes[:,1]), 2))\n",
    "    h = torch.sqrt(torch.pow((bboxes[:,6] -bboxes[:,0]), 2) + torch.pow((bboxes[:,7] -bboxes[:,1]), 2))\n",
    "    grasps = torch.stack((x, y, theta, h, w), 1)\n",
    "    return grasps\n",
    "\n",
    "def grasps_to_bboxes(grasps):\n",
    "    # convert grasp representation to bbox\n",
    "    x = grasps[:,0]\n",
    "    y = grasps[:,1]\n",
    "    theta = grasps[:,2]\n",
    "    h = grasps[:,3]\n",
    "    w = grasps[:,4]\n",
    "    x1 = x -w/2*torch.cos(theta) +h/2*torch.sin(theta)\n",
    "    y1 = y -w/2*torch.sin(theta) -h/2*torch.cos(theta)\n",
    "    x2 = x +w/2*torch.cos(theta) +h/2*torch.sin(theta)\n",
    "    y2 = y +w/2*torch.sin(theta) -h/2*torch.cos(theta)\n",
    "    x3 = x +w/2*torch.cos(theta) -h/2*torch.sin(theta)\n",
    "    y3 = y +w/2*torch.sin(theta) +h/2*torch.cos(theta)\n",
    "    x4 = x -w/2*torch.cos(theta) -h/2*torch.sin(theta)\n",
    "    y4 = y -w/2*torch.sin(theta) +h/2*torch.cos(theta)\n",
    "    bboxes = torch.stack((x1, y1, x2, y2, x3, y3, x4, y4), 1)\n",
    "    return bboxes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test transform function\n",
    "for img, label in train_dl:\n",
    "    print(label)\n",
    "    print(label.shape)\n",
    "    \n",
    "    grasps = bboxes_to_grasps(label)\n",
    "    print(grasps)\n",
    "    print(grasps.shape)\n",
    "    \n",
    "    x = grasps_to_bboxes(grasps)\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    \n",
    "    y = bboxes_to_grasps(x)\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(img[0], label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(img[0], x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from shapely.geometry import Polygon\n",
    "from math import pi \n",
    "\n",
    "def box_iou(bbox_value, bbox_target):\n",
    "    p1 = Polygon(bbox_value.view(-1,2).tolist())\n",
    "    p2 = Polygon(bbox_target.view(-1,2).tolist())\n",
    "    iou = p1.intersection(p2).area / (p1.area +p2.area -p1.intersection(p2).area) \n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "def metrics_batch(output, target):\n",
    "    pre_bboxes = grasps_to_bboxes(output)\n",
    "    count = 0\n",
    "    if target.shape[1] == 8:\n",
    "        target_grasps = bboxes_to_grasps(target)  \n",
    "        for i in range(len(output)):  \n",
    "            iou = box_iou(pre_bboxes[i], target[i])\n",
    "            pre_theta = output[i][2]\n",
    "            target_theta = target_grasps[i][2]\n",
    "            angle_diff = torch.abs(pre_theta - target_theta)*180/pi\n",
    "            if angle_diff < 30 and iou > 0.001:\n",
    "                count = count + 1\n",
    "    else:\n",
    "        good = [0 for i in range(len(output))]\n",
    "        all_grasps = bboxes_to_grasps(target[:, 1:])\n",
    "        for i in range(len(target)):\n",
    "            index = target[i][0].int()\n",
    "            if good[index] == 1:\n",
    "                continue\n",
    "            iou = box_iou(pre_bboxes[index], target[i][1:])\n",
    "            pre_theta = output[index][2]\n",
    "            target_theta = all_grasps[i][2]\n",
    "            angle_diff = torch.abs(pre_theta - target_theta)*180/pi\n",
    "            if angle_diff < 30 and iou > 0.001:\n",
    "                good[index] = 1\n",
    "        for flag in good:\n",
    "            if flag == 1:\n",
    "                count = count + 1\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in train_dl:\n",
    "    grasps = bboxes_to_grasps(label)\n",
    "    bboxes = grasps_to_bboxes(grasps)\n",
    "    print(box_iou(label[0], bboxes[0]))   \n",
    "    print(metrics_batch(grasps, label))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in val_dl:\n",
    "    bboxes = torch.rand(img.shape[0], 8)\n",
    "    for box in label:\n",
    "        bboxes[box[0].int()] = box[1:]\n",
    "    grasps = bboxes_to_grasps(bboxes)\n",
    "    bboxes_new = grasps_to_bboxes(grasps)\n",
    "    print(box_iou(bboxes_new[0], bboxes[0]))   \n",
    "    print(metrics_batch(grasps, label))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loss_batch(output,targets, params_loss, opt=None):  \n",
    "    mse_loss = params_loss[\"mse_loss\"]\n",
    "    gama = params_loss[\"gama\"]\n",
    "    grasps = bboxes_to_grasps(targets)\n",
    "    loss_x =mse_loss(output[:,0], grasps[:,0])\n",
    "    loss_y =mse_loss(output[:,1], grasps[:,1])\n",
    "    loss_h =mse_loss(output[:,3], grasps[:,3])\n",
    "    loss_w =mse_loss(output[:,4], grasps[:,4])\n",
    "    loss_theta = mse_loss(output[:,2], grasps[:,2])\n",
    "    loss = loss_x + loss_y + loss_h + loss_w + gama*loss_theta\n",
    "        \n",
    "    # get performance metric\n",
    "    metric_b = metrics_batch(output,targets)\n",
    "    \n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return loss.item(), metric_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1.0,3], [1, 1, ]])\n",
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss_batch(output,targets, params_loss):  \n",
    "    loss = 0.0\n",
    "    gama = params_loss[\"gama\"]\n",
    "    \n",
    "    for index, grasp in enumerate(output):\n",
    "        target = [g[1:] for g in targets if g[0].int() == index]\n",
    "        target = torch.stack(target)\n",
    "        target_grasps = bboxes_to_grasps(target)\n",
    "        loss_x = torch.pow((target_grasps[:,0] - grasp[0]), 2).mean()\n",
    "        loss_y = torch.pow((target_grasps[:,1] - grasp[1]), 2).mean()\n",
    "        loss_h = torch.pow((target_grasps[:,3] - grasp[3]), 2).mean()\n",
    "        loss_w = torch.pow((target_grasps[:,4] - grasp[4]), 2).mean()\n",
    "        loss_theta = torch.pow((target_grasps[:,2] - grasp[2]), 2).mean()\n",
    "        loss = loss + loss_x + loss_y + loss_h + loss_w + gama*loss_theta\n",
    "        \n",
    "    # get performance metric\n",
    "    metric_b = metrics_batch(output,targets)\n",
    "    \n",
    "\n",
    "    return loss, metric_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_epoch(model, params_loss, dataset_dl, sanity_check=False, opt=None, training=True):\n",
    "    running_loss=0.0\n",
    "    running_metric=0.0\n",
    "    len_data=len(dataset_dl.dataset)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        yb=yb.to(device)\n",
    "        \n",
    "        # get model output\n",
    "        output=model(xb.to(device))\n",
    "        \n",
    "        # get loss per batch\n",
    "        if training:\n",
    "            loss_b,metric_b = train_loss_batch(output, yb, params_loss, opt)\n",
    "        else:\n",
    "            loss_b,metric_b = val_loss_batch(output, yb, params_loss)\n",
    "        \n",
    "        # update running loss\n",
    "        running_loss+=loss_b\n",
    "        \n",
    "        # update running metric\n",
    "        running_metric += metric_b\n",
    "        \n",
    "        if sanity_check:\n",
    "            break\n",
    "\n",
    "    # average loss value\n",
    "    loss = running_loss/float(len_data)\n",
    "    \n",
    "    # average metric value\n",
    "    metric = running_metric/float(len_data)\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "opt = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "import copy\n",
    "def train_val(model, params):\n",
    "    num_epochs=params[\"num_epochs\"]\n",
    "    params_loss=params[\"params_loss\"]\n",
    "    opt=params[\"optimizer\"]\n",
    "    train_dl=params[\"train_dl\"]\n",
    "    val_dl=params[\"val_dl\"]\n",
    "    sanity_check=params[\"sanity_check\"]\n",
    "    lr_scheduler=params[\"lr_scheduler\"]\n",
    "    path2weights=params[\"path2weights\"]\n",
    "    \n",
    "    # history of loss values in each epoch\n",
    "    loss_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    \n",
    "    # histroy of metric values in each epoch\n",
    "    metric_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }    \n",
    "    \n",
    "    \n",
    "    # a deep copy of weights for the best performing model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # initialize best loss to a large value\n",
    "    best_loss=float('inf')    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # get current learning rate\n",
    "        current_lr=get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr))   \n",
    "\n",
    "        # train the model\n",
    "        model.train()\n",
    "        train_loss, train_metric=loss_epoch(model,params_loss,train_dl,sanity_check,opt)\n",
    "\n",
    "        # collect loss and metric for training dataset\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        metric_history[\"train\"].append(train_metric)\n",
    "        \n",
    "        # evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric=loss_epoch(model,params_loss,val_dl,sanity_check, training=False)\n",
    "       \n",
    "        # collect loss and metric for validation dataset\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        metric_history[\"val\"].append(val_metric)   \n",
    "        \n",
    "        \n",
    "        # store best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # store weights into a local file\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print(\"Copied best model weights!\")\n",
    "            \n",
    "        # learning rate schedule\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print(\"Loading best model weights!\")\n",
    "            model.load_state_dict(best_model_wts) \n",
    "            \n",
    "\n",
    "        print(\"train loss: %.6f, accuracy: %.4f\" %(train_loss*(40/11),100*train_metric))\n",
    "        print(\"val loss: %.6f, accuracy: %.4f\" %(val_loss*(40/11),100*val_metric))\n",
    "        print(\"-\"*10) \n",
    "        \n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)\n",
    "\n",
    "path2models= \"../models/\"\n",
    "if not os.path.exists(path2models):\n",
    "        os.mkdir(path2models)\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "params_loss={\n",
    "    \"mse_loss\": mse_loss,\n",
    "    \"gama\": 5.0,\n",
    "}\n",
    "\n",
    "params_train={\n",
    "    \"num_epochs\": 100,\n",
    "    \"optimizer\": opt,\n",
    "    \"params_loss\": params_loss,\n",
    "    \"train_dl\": train_dl,\n",
    "    \"val_dl\": val_dl,\n",
    "    \"sanity_check\": True,\n",
    "    \"lr_scheduler\": lr_scheduler,\n",
    "    \"path2weights\": path2models+\"weights.pt\",\n",
    "}\n",
    "\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "model, loss_hist, metric_history = train_val(model, params_train)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to execute train_val: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation Progress\n",
    "num_epochs=params_train[\"num_epochs\"]\n",
    "\n",
    "# plot loss progress\n",
    "plt.title(\"Train-Val Loss\")\n",
    "plt.plot(range(1,num_epochs+1),loss_hist[\"train\"],label=\"train\")\n",
    "plt.plot(range(1,num_epochs+1),loss_hist[\"val\"],label=\"val\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "#plt.xticks(range(1, num_epochs+1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy progress\n",
    "plt.title(\"Train-Val Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),metric_history[\"train\"],label=\"train\")\n",
    "plt.plot(range(1,num_epochs+1),metric_history[\"val\"],label=\"val\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "#plt.xticks(range(1, num_epochs+1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
